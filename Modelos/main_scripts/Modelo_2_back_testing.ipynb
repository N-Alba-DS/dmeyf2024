{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precarga de librerias y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from shap) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\.conda\\envs\\datascience\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Admin\\.conda\\envs\\datascience\\Lib\\site-packages\\pandas-2.2.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "%run \"../recurrentes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../funciones.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preseteo optuna especificar bucket y nombre del estudio\n",
    "\n",
    "\n",
    "# nombre_archivo = 'optimization_tree.db'\n",
    "# bucket = 'b2/'\n",
    "\n",
    "\n",
    "# estudio_optuna = base_path + 'buckets/' + bucket + 'optimization_tree.db'\n",
    "\n",
    "\n",
    "# # cargar estudio\n",
    "# # a) competencia_02\n",
    "# # b) competencia_02_lags\n",
    "# # c) competencia_02_lags_y_deltas\n",
    "\n",
    "# nombre_estudio = 'competencia_02_lags'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activar Excel de seguimiento de medidas? \n",
    "\n",
    "excel = True\n",
    "if excel == True:\n",
    "    # Ingresar path excel para anotar resultados\n",
    "    path_excel = base_path_l + r'\\resultados_backtesting.xlsx'\n",
    "    date = datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Cargar datos\n",
    "# Opciones:\n",
    "# a) dataset_clase_ternaria_l\n",
    "# b) dataset_lags_clase_ternaria_l\n",
    "# c) dataset_lags_deltas_y_clase_ternaria_l\n",
    "\n",
    "\n",
    "dataset = dataset_lags_deltas_y_clase_ternaria_l\n",
    "dataset_name = os.path.basename(dataset)\n",
    "df_train = pd.read_parquet(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formateo pre modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'T_Visa_normal' in df_train.columns:\n",
    "    df_train['T_Visa_normal'] = df_train['T_Visa_normal'].astype(bool)\n",
    "if 'T_Master_normal'in df_train.columns:\n",
    "    df_train['T_Master_normal'] = df_train['T_Master_normal'].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - parametros para modelo\n",
    "ganancia_acierto = 273000\n",
    "costo_estimulo = 7000\n",
    "\n",
    "mes_train = 202104\n",
    "mes_test = 202106\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4735593, 600)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_train\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['clase_peso'] = 1.0\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clase_binaria'] = np.where(data['clase_ternaria']=='CONTINUA', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar mes de train y test\n",
    "\n",
    "df_train = data[data['foto_mes']<=mes_train]\n",
    "df_test = data[data['foto_mes']==mes_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4735593, 602)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4735593 entries, 0 to 4735592\n",
      "Columns: 602 entries, numero_de_cliente to clase_binaria\n",
      "dtypes: bool(2), float64(535), int32(1), int64(63), object(1)\n",
      "memory usage: 21.4 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)  # Número de filas y columnas\n",
    "print(data.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clase_peso = df_train['clase_peso']\n",
    "X_train = df_train.drop(['clase_ternaria', 'clase_binaria', 'clase_peso'], axis=1)\n",
    "Y_train =df_train['clase_binaria']\n",
    "X_test = df_test.drop(['clase_ternaria', 'clase_binaria', 'clase_peso'], axis=1)\n",
    "Y_test =df_test['clase_binaria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x15defdd9290>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_test = df_test.loc[Y_test.index, 'clase_peso']\n",
    "test_data = lgb.Dataset(X_test, label=Y_test, weight=w_test)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = df_train.loc[X_train.index, 'clase_peso']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar entradas en df_train en base a la columna foto_mes de meses más antiguos a más recientes\n",
    "X_train = X_train.sort_values('foto_mes', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[400009, 500009, 500011, 500021, 600009]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7210, number of negative: 672040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 679250, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 76290, number of negative: 1282210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 1358500, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 294796, number of negative: 1742954\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.683766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2037750, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 299732, number of negative: 2417268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2717000, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 597781, number of negative: 2798469\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.291444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 3396250, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010615 -> initscore=-4.534834\n",
      "[LightGBM] [Info] Start training from score -4.534834\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056159 -> initscore=-2.821779\n",
      "[LightGBM] [Info] Start training from score -2.821779\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144669 -> initscore=-1.777038\n",
      "[LightGBM] [Info] Start training from score -1.777038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110319 -> initscore=-2.087490\n",
      "[LightGBM] [Info] Start training from score -2.087490\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176014 -> initscore=-1.543588\n",
      "[LightGBM] [Info] Start training from score -1.543588\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "folds = tscv.split(X_train)\n",
    "\n",
    "lista_cv = []\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  # Puedes cambiar esto si tu problema es multiclase u otro tipo\n",
    "    'metric': 'binary_logloss',  # Cambia el metric si es necesario\n",
    "    }\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pasar las divisiones a lgb.cv\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=300,\n",
    "    feval=lgb_gan_eval,\n",
    "    folds=folds,  # Aquí especificamos las divisiones temporales\n",
    "    seed=s,\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "final =  max_gan * 5\n",
    "lista_cv.append(final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7210, number of negative: 672040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 679250, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 76290, number of negative: 1282210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 1358500, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 294796, number of negative: 1742954\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.721174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2037750, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 299732, number of negative: 2417268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.278071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2717000, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 597781, number of negative: 2798469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.007719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 3396250, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010615 -> initscore=-4.534834\n",
      "[LightGBM] [Info] Start training from score -4.534834\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056159 -> initscore=-2.821779\n",
      "[LightGBM] [Info] Start training from score -2.821779\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144669 -> initscore=-1.777038\n",
      "[LightGBM] [Info] Start training from score -1.777038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110319 -> initscore=-2.087490\n",
      "[LightGBM] [Info] Start training from score -2.087490\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176014 -> initscore=-1.543588\n",
      "[LightGBM] [Info] Start training from score -1.543588\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "folds = tscv.split(X_train)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  # Puedes cambiar esto si tu problema es multiclase u otro tipo\n",
    "    'metric': 'binary_logloss',  # Cambia el metric si es necesario\n",
    "    }\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pasar las divisiones a lgb.cv\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=300,\n",
    "    feval=lgb_gan_eval,\n",
    "    folds=folds,  # Aquí especificamos las divisiones temporales\n",
    "    seed=semillas[1],\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "final_1 =  max_gan * 5\n",
    "lista_cv.append(final_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7210, number of negative: 672040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 679250, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 76290, number of negative: 1282210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 1358500, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 294796, number of negative: 1742954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2037750, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 299732, number of negative: 2417268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.278097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2717000, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 597781, number of negative: 2798469\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 3396250, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010615 -> initscore=-4.534834\n",
      "[LightGBM] [Info] Start training from score -4.534834\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056159 -> initscore=-2.821779\n",
      "[LightGBM] [Info] Start training from score -2.821779\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144669 -> initscore=-1.777038\n",
      "[LightGBM] [Info] Start training from score -1.777038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110319 -> initscore=-2.087490\n",
      "[LightGBM] [Info] Start training from score -2.087490\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176014 -> initscore=-1.543588\n",
      "[LightGBM] [Info] Start training from score -1.543588\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "folds = tscv.split(X_train)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  # Puedes cambiar esto si tu problema es multiclase u otro tipo\n",
    "    'metric': 'binary_logloss',  # Cambia el metric si es necesario\n",
    "    }\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pasar las divisiones a lgb.cv\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=300,\n",
    "    feval=lgb_gan_eval,\n",
    "    folds=folds,  # Aquí especificamos las divisiones temporales\n",
    "    seed=semillas[2],\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "final_2 =  max_gan * 5\n",
    "lista_cv.append(final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7210, number of negative: 672040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 679250, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 76290, number of negative: 1282210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 1358500, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 294796, number of negative: 1742954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2037750, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 299732, number of negative: 2417268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.242960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2717000, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 597781, number of negative: 2798469\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 3396250, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010615 -> initscore=-4.534834\n",
      "[LightGBM] [Info] Start training from score -4.534834\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056159 -> initscore=-2.821779\n",
      "[LightGBM] [Info] Start training from score -2.821779\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144669 -> initscore=-1.777038\n",
      "[LightGBM] [Info] Start training from score -1.777038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110319 -> initscore=-2.087490\n",
      "[LightGBM] [Info] Start training from score -2.087490\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176014 -> initscore=-1.543588\n",
      "[LightGBM] [Info] Start training from score -1.543588\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "folds = tscv.split(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  # Puedes cambiar esto si tu problema es multiclase u otro tipo\n",
    "    'metric': 'binary_logloss',  # Cambia el metric si es necesario\n",
    "    }\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pasar las divisiones a lgb.cv\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=300,\n",
    "    feval=lgb_gan_eval,\n",
    "    folds=folds,  # Aquí especificamos las divisiones temporales\n",
    "    seed=semillas[3],\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "final_3 =  max_gan * 5\n",
    "lista_cv.append(final_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7210, number of negative: 672040\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 679250, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 76290, number of negative: 1282210\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.440371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 1358500, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 294796, number of negative: 1742954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2037750, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 299732, number of negative: 2417268\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.282642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 2717000, number of used features: 154\n",
      "[LightGBM] [Info] Number of positive: 597781, number of negative: 2798469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.115082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22081\n",
      "[LightGBM] [Info] Number of data points in the train set: 3396250, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010615 -> initscore=-4.534834\n",
      "[LightGBM] [Info] Start training from score -4.534834\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056159 -> initscore=-2.821779\n",
      "[LightGBM] [Info] Start training from score -2.821779\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144669 -> initscore=-1.777038\n",
      "[LightGBM] [Info] Start training from score -1.777038\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110319 -> initscore=-2.087490\n",
      "[LightGBM] [Info] Start training from score -2.087490\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176014 -> initscore=-1.543588\n",
      "[LightGBM] [Info] Start training from score -1.543588\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "folds = tscv.split(X_train)\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  # Puedes cambiar esto si tu problema es multiclase u otro tipo\n",
    "    'metric': 'binary_logloss',  # Cambia el metric si es necesario\n",
    "    }\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pasar las divisiones a lgb.cv\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=300,\n",
    "    feval=lgb_gan_eval,\n",
    "    folds=folds,  # Aquí especificamos las divisiones temporales\n",
    "    seed=semillas[4],\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "final_4 =  max_gan * 5\n",
    "lista_cv.append(final_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10737406704.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalisimo =  sum(lista_cv)/ len(lista_cv)  \n",
    "finalisimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTADOS:\n",
    "# 1. Modelo Base = 10737406704.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción y scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_medidas = [accuracy_score, precision_score, recall_score, f1_score, roc_auc_score]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=Y_train, weight=w_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo utilizado para back testing utilizando parametros default si la diferencia en CV es mínima intentar optuna y luego \n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',  \n",
    "    'metric': 'binary_logloss',  \n",
    "    'seed': semillas[0], }\n",
    "model_default = lgb.train(params, train_data)    \n",
    "y_pred_default = model_default.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_medidas = {}\n",
    "if excel == True:\n",
    "    dicc_medidas['fecha'] = date\n",
    "    dicc_medidas['dataset'] = dataset_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = (y_pred_default >= 0.025).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fecha': '18-11-2024-', 'dataset': 'competencia_02_lags_y_clase_ternaria.parquet', 'ganancia': ('gan_eval', 83524000, True), 'accuracy_score': 0.9229966762900604, 'precision_score': 0.09640461294942339, 'recall_score': 0.6436839456467036, 'f1_score': 0.16769371968008392, 'roc_auc_score': 0.7850439078834234}\n"
     ]
    }
   ],
   "source": [
    "for medida in lista_medidas:\n",
    "     dicc_medidas[medida.__name__] = medida(Y_test, y_pred_labels)\n",
    "print(dicc_medidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganancia = lgb_gan_eval(y_pred_default, test_data)\n",
    "dicc_medidas['ganancia'] = ganancia[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_columnas = df_train.shape[1]\n",
    "mes_train = df_train['foto_mes'].max()\n",
    "mes_test = df_test['foto_mes'].max()\n",
    "\n",
    "dicc_medidas['cantidad_columnas'] = cantidad_columnas\n",
    "dicc_medidas['mes_train'] = mes_train\n",
    "dicc_medidas['mes_test'] = mes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fecha': '18-11-2024-',\n",
       " 'dataset': 'competencia_02_lags_y_clase_ternaria.parquet',\n",
       " 'ganancia': 83524000,\n",
       " 'accuracy_score': 0.9229966762900604,\n",
       " 'precision_score': 0.09640461294942339,\n",
       " 'recall_score': 0.6436839456467036,\n",
       " 'f1_score': 0.16769371968008392,\n",
       " 'roc_auc_score': 0.7850439078834234}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_medidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de columnas: {'fecha': 1, 'dataset': 2, 'ganancia': 3, 'accuracy_score': 4, 'precision_score': 5, 'recall_score': 6, 'f1_score': 7, 'roc_auc_score': 8}\n",
      "Fila a actualizar: 2\n"
     ]
    }
   ],
   "source": [
    "if excel == True:\n",
    "    \n",
    "    wb = load_workbook(path_excel)\n",
    "    ws = wb.active\n",
    "\n",
    "    header_row = 1  \n",
    "    column_mapping = {cell.value: cell.column for cell in ws[header_row]}\n",
    "\n",
    "    # Verificar el mapeo de columnas\n",
    "    print(\"Mapeo de columnas:\", column_mapping)\n",
    "    \n",
    "row_to_update = None\n",
    "\n",
    "for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=1):\n",
    "    cell = row[0]  # La primera celda de la fila\n",
    "    if cell.value is None:  # Si el valor de la celda está vacía\n",
    "        print(f\"Primera celda vacía en la fila {cell.row}\")\n",
    "        row_to_update = cell.row\n",
    "        break\n",
    "\n",
    "# Si no se encontró ninguna celda vacía, agregar al final\n",
    "if row_to_update is None:\n",
    "    print(\"No se encontro celda vacía, agregando al final\")\n",
    "    row_to_update = ws.max_row + 1  # Agregar en una nueva fila al final\n",
    "\n",
    "print(f\"Fila a actualizar: {row_to_update}\")\n",
    "fila_actualizar = row_to_update  # Fila donde escribir los datos\n",
    "\n",
    "for col_name, value in dicc_medidas.items():\n",
    "    if col_name in column_mapping:  # Asegurarse de que la columna existe\n",
    "        col_idx = column_mapping[col_name]\n",
    "        ws.cell(row=fila_actualizar, column=col_idx, value=value)\n",
    "\n",
    "# Guardar los cambios\n",
    "wb.save(path_excel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
